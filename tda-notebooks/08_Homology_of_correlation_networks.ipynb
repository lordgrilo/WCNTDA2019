{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistent homology of correlation networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "import numpy as np \n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use some fun data!   \n",
    "In the directory below you find timeseries for a 100 subjects for two recordings. \n",
    "They have been studied in [this](http://www.gipsa-lab.grenoble-inp.fr/~sophie.achard/Brain_connectivity_network) paper about test-retest reliability of brain functional networks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here you can find timeseries for a 100 subjects for test and retest \n",
    "data_dir = '../data/corr-mats/TimeSeriesAAL/'\n",
    "\n",
    "fs = os.listdir(data_dir)\n",
    "ts_data = {}\n",
    "subs = []\n",
    "for f in fs:\n",
    "    if f[0]!='.':\n",
    "        sub = f.split('_')[1]\n",
    "        subs.append(sub)\n",
    "        test = int(f.split('_')[3][-1]);\n",
    "        if sub not in ts_data:\n",
    "            ts_data[sub] = {}\n",
    "        ts_data[sub][test] = np.loadtxt(data_dir+f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.DS_Store']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate correlation matrices the easy way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dict = {}\n",
    "\n",
    "for sub in ts_data:\n",
    "    corr_dict[sub] = {}\n",
    "    for test in ts_data[sub]:\n",
    "        corr_dict[sub][test] = np.corrcoef(ts_data[sub][test].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ripser import Rips\n",
    "from persim import PersImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rips(maxdim=1, thresh=2, coeff=2, do_cocycles=False, n_perm = None, verbose=True)\n"
     ]
    }
   ],
   "source": [
    "rips = Rips(maxdim=1, coeff=2, do_cocycles=False,thresh=2);\n",
    "diagrams_h1 = []\n",
    "labels = []\n",
    "for test in [0,1]:\n",
    "    for i,sub in enumerate(corr_dict):\n",
    "        labels.append(test);\n",
    "        # note that we use 1-corr because these are pearson correlations and hence \n",
    "        # this is a proper metric\n",
    "        dist = 1.0 - corr_dict[sub][test]\n",
    "        diagrams_h1.append(rips.fit_transform(dist,distance_matrix=True)[1]) #don't forget the distance matrix flag!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ffb81e18be0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mplot_diagrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiagrams_h1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i in range(100):\n",
    "    plt.subplot(10,10,i+1)\n",
    "    plot_diagrams(diagrams_h1[i])\n",
    "    plt.ylim(0,1)\n",
    "    plt.xlim(0,1)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagrams_h1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgms = {}\n",
    "for i,sub in enumerate(corr_dict):\n",
    "    dgms[sub] = []\n",
    "    for test in [0,1]:\n",
    "        dgms[sub].append(rips.fit_transform(1.0 - corr_dict[sub][test],distance_matrix=True)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wasserstein distances inter- and intra-subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations;\n",
    "inter_dist = {}\n",
    "intra_dist = {}\n",
    "\n",
    "for s, ss in combinations(corr_dict.keys(),2):\n",
    "    if s not in intra_dist:\n",
    "        intra_dist[s] = persim.sliced_wasserstein(dgms[s][0],dgms[s][1],300);\n",
    "        inter_dist[s] = []\n",
    "    if ss not in intra_dist:\n",
    "        intra_dist[ss] = persim.sliced_wasserstein(dgms[ss][0],dgms[ss][1],300);\n",
    "        inter_dist[ss] = [];\n",
    "    d = persim.sliced_wasserstein(dgms[s][0],dgms[ss][1],300);\n",
    "    inter_dist[s].append(d);\n",
    "    d = persim.sliced_wasserstein(dgms[s][1],dgms[ss][0],300);\n",
    "    inter_dist[ss].append(d);\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  list(map(lambda x: intra_dist[x], corr_dict.keys()))\n",
    "y = list(map(lambda x: np.mean(inter_dist[x]), corr_dict.keys()))\n",
    "yerr = list(map(lambda x: np.std(inter_dist[x]), corr_dict.keys()))\n",
    "\n",
    "plt.plot(x,y,'o')\n",
    "plt.errorbar(x,y,yerr,fmt='.')\n",
    "plt.plot(np.linspace(0,1,10), np.linspace(0,1,10),'k--')\n",
    "print(np.sum([np.array(x)<np.array(y)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottleneck distances inter- and intra-subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_dist_b = {}\n",
    "intra_dist_b = {}\n",
    "\n",
    "for s, ss in combinations(corr_dict.keys(),2):\n",
    "    if s not in intra_dist_b:\n",
    "        intra_dist_b[s] = persim.bottleneck(dgms[s][0],dgms[s][1]);\n",
    "        inter_dist_b[s] = []\n",
    "    if ss not in intra_dist_b:\n",
    "        intra_dist_b[ss] = persim.bottleneck(dgms[ss][0],dgms[ss][1]);\n",
    "        inter_dist_b[ss] = [];\n",
    "    d = persim.bottleneck(dgms[s][0],dgms[ss][1]);\n",
    "    inter_dist_b[s].append(d);\n",
    "    d = persim.bottleneck(dgms[s][1],dgms[ss][0]);\n",
    "    inter_dist_b[ss].append(d);\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  list(map(lambda x: intra_dist_b[x], corr_dict.keys()))\n",
    "y = list(map(lambda x: np.mean(inter_dist_b[x]), corr_dict.keys()))\n",
    "yerr = list(map(lambda x: np.std(inter_dist_b[x]), corr_dict.keys()))\n",
    "\n",
    "plt.plot(x,y,'o')\n",
    "plt.errorbar(x,y,yerr,fmt='.')\n",
    "plt.plot(np.linspace(0,0.09,10), np.linspace(0,0.09,10),'k--')\n",
    "print(np.sum([np.array(x)<np.array(y)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heat kernel sims inter- and intra-subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_dist_h = {}\n",
    "intra_dist_h = {}\n",
    "\n",
    "for s, ss in combinations(corr_dict.keys(),2):\n",
    "    if s not in intra_dist_h:\n",
    "        intra_dist_h[s] = persim.heat(dgms[s][0],dgms[s][1]);\n",
    "        inter_dist_h[s] = []\n",
    "    if ss not in intra_dist_h:\n",
    "        intra_dist_h[ss] = persim.heat(dgms[ss][0],dgms[ss][1]);\n",
    "        inter_dist_h[ss] = [];\n",
    "    d = persim.heat(dgms[s][0],dgms[ss][1]);\n",
    "    inter_dist_h[s].append(d);\n",
    "    d = persim.heat(dgms[s][1],dgms[ss][0]);\n",
    "    inter_dist_h[ss].append(d);\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  list(map(lambda x: intra_dist_h[x], corr_dict.keys()))\n",
    "y = list(map(lambda x: np.mean(inter_dist_h[x]), corr_dict.keys()))\n",
    "yerr = list(map(lambda x: np.std(inter_dist_h[x]), corr_dict.keys()))\n",
    "\n",
    "plt.plot(x,y,'o')\n",
    "plt.errorbar(x,y,yerr,fmt='.')\n",
    "plt.plot(np.linspace(0,0.09,10), np.linspace(0,0.09,10),'k--')\n",
    "print(np.sum([np.array(x)<np.array(y)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_dist_hr = {}\n",
    "intra_dist_hr = {}\n",
    "\n",
    "self_hr = {}\n",
    "for sub in subs:\n",
    "    self_hr[sub] = list(map(lambda x: persim.heat(dgms[sub][x], dgms[sub][x]), [0,1]))\n",
    "\n",
    "for s, ss in combinations(subs,2):\n",
    "    if s not in intra_dist_hr:\n",
    "        intra_dist_hr[s] = 2.0* persim.heat(dgms[s][0],dgms[s][1])/(self_hr[s][0] + self_hr[s][1]);\n",
    "        inter_dist_hr[s] = []\n",
    "    if ss not in intra_dist_hr:\n",
    "        intra_dist_hr[ss] = 2.0*persim.heat(dgms[ss][0],dgms[ss][1])/(self_hr[ss][0] + self_hr[ss][1]);\n",
    "        inter_dist_hr[ss] = [];\n",
    "    d = 2.0*persim.heat(dgms[s][0],dgms[ss][1])/(self_hr[s][0] + self_hr[ss][1]);\n",
    "    inter_dist_hr[s].append(d);\n",
    "    d = 2.0 * persim.heat(dgms[s][1],dgms[ss][0])/(self_hr[s][1] + self_hr[ss][0]);\n",
    "    inter_dist_hr[ss].append(d);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =  list(map(lambda x: intra_dist_hr[x], subs))\n",
    "y = list(map(lambda x: np.mean(inter_dist_hr[x]),subs))\n",
    "yerr = list(map(lambda x: np.std(inter_dist_hr[x]), subs))\n",
    "\n",
    "plt.plot(x,y,'o')\n",
    "plt.errorbar(x,y,yerr,fmt='.')\n",
    "plt.plot(np.linspace(0,0.09,10), np.linspace(0,0.09,10),'k--')\n",
    "print(np.sum([np.array(x)<np.array(y)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pim = PersImage(pixels=([30,30]), spread=None)\n",
    "imgs = pim.transform(diagrams_h1)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "for i in range(100):\n",
    "    plt.subplot(10,10,i+1)\n",
    "    pim.show(imgs[i])\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "imgs_array = np.array([img.flatten() for img in imgs])\n",
    "X_train, X_test, y_train, y_test = train_test_split(imgs_array, labels, test_size=0.40, random_state=42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "print(lr.score(X_test, y_test))\n",
    "\n",
    "inverse_image = np.copy(lr.coef_).reshape((30,30))\n",
    "plt.figure()\n",
    "pim.show(inverse_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
